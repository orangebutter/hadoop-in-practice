package cn.dataguru.hadoop;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.MapReduceBase;

import org.apache.hadoop.mapred.*;
import org.apache.hadoop.util.*;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.*;
import java.util.Map.Entry;

//位置数据
//IMSI|IMEI|UPDATETYPE|CGI|TIME
//上网数据
//IMSI|IMEI|CGI|TIME|CGI|URL

/**
 * 汇总基站数据表
 * 计算每个用户在不同的时间段不同的基站停留的时长
 * 输入参数 < input path > < output path > < date > < timepoint >
 * 参数示例： “/base /output 2012-09-12 09-17-24"
 * 意味着以“/base”为输入，"/output"为输出，指定计算2012年09月12日的数据，并分为00-07，07-17，17-24三个时段
 * 输出格式 “IMSI|CGI|TIMFLAG|STAY_TIME”
 */
public class BaseStationDataPreprocess extends Configured implements Tool {
    /**
     * 计数器
     * 用于计数各种异常数据
     */
    enum Counter {
        TIMESKIP,        //时间格式有误
        OUTOFTIMESKIP,    //时间不在参数指定的时间段内
        LINESKIP,        //源文件行有误
        USERSKIP        //某个用户某个时间段被整个放弃
    }

    /**
     * 读取一行数据
     * 以“IMSI+时间段”作为 KEY 发射出去
     */
    public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, Text> {
        private String date;
        private String[] timepoint;
        private boolean dataSource;

        /**
         * 初始化
         */
        @Override
        public void configure(org.apache.hadoop.mapred.JobConf job) {
            this.date = job.get("date");                            //读取日期
            this.timepoint = job.get("timepoint").split("-");    //读取时间分割点

            //提取文件名
//			FileSplit fs = (FileSplit)job.getInputSplit();
            String fileName = job.get("map.input.file");
            if (fileName.startsWith("POS"))
                dataSource = true;
            else if (fileName.startsWith("NET"))
                dataSource = false;

        }


        /**
         * MAP任务
         * 读取基站数据
         * 找出数据所对应时间段
         * 以IMSI和时间段作为 KEY
         * CGI和时间作为 VALUE
         */
        @Override
        public void map(LongWritable key, Text value, OutputCollector<Text, Text> outputCollector, Reporter reporter) throws IOException {
            String line = value.toString();
            TableLine tableLine = new TableLine();

            //读取行
            try {
                tableLine.set(line, this.dataSource, this.date, this.timepoint);
            } catch (LineException e) {
                if (e.getFlag() == -1)
                    reporter.getCounter(Counter.OUTOFTIMESKIP).increment(1);
                else
                    reporter.getCounter(Counter.TIMESKIP).increment(1);
                return;
            } catch (Exception e) {
                reporter.getCounter(Counter.LINESKIP).increment(1);
                return;
            }

            outputCollector.collect(tableLine.outKey(), tableLine.outValue());
        }
    }


    /**
     * 统计同一个IMSI在同一时间段
     * 在不同CGI停留的时长
     */
    public static class Reduce extends MapReduceBase implements Reducer<Text, Text, NullWritable, Text> {
        private String date;
        private SimpleDateFormat formatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");

        /**
         * 初始化
         */
        @Override
        public void configure(org.apache.hadoop.mapred.JobConf job) {
            this.date = job.get("date");
        }

        public void reduce(Text key, Iterator<Text> values, OutputCollector<NullWritable, Text> output, Reporter reporter) throws IOException {
            String imsi = key.toString().split("\\|")[0];
            String timeFlag = key.toString().split("\\|")[1];

            //用一个TreeMap记录时间
            TreeMap<Long, String> uploads = new TreeMap<Long, String>();
            String valueString;

            while (values.hasNext()) {
                valueString = values.next().toString();
                try {
                    uploads.put(Long.valueOf(valueString.split("\\|")[1]), valueString.split("\\|")[0]);
                } catch (NumberFormatException e) {
                    reporter.getCounter(Counter.TIMESKIP).increment(1);
                    continue;
                }
            }

            try {
                //在最后添加“OFF”位置
                Date tmp = this.formatter.parse(this.date + " " + timeFlag.split("-")[1] + ":00:00");
                uploads.put((tmp.getTime() / 1000L), "OFF");

                //汇总数据
                HashMap<String, Float> locs = getStayTime3(uploads);

                //输出
                for (Entry<String, Float> entry : locs.entrySet()) {
                    StringBuilder builder = new StringBuilder();
                    builder.append(imsi).append("|");
                    builder.append(entry.getKey()).append("|");
                    builder.append(timeFlag).append("|");
                    builder.append(entry.getValue());

                    output.collect(NullWritable.get(), new Text(builder.toString()));
                }
            } catch (Exception e) {
                reporter.getCounter(Counter.USERSKIP).increment(1);
                return;
            }
        }
        /**
         * 获得位置前三个停留信息
         */
        private HashMap<String, Float> getStayTime3(TreeMap<Long, String> uploads) {
            HashMap<String,Float> locs = getStayTime(uploads);
            HashMap<String,Float> result = new HashMap();
            List<String> posList = new ArrayList<String>();
            for(Iterator<String> keyIt = locs.keySet().iterator();keyIt.hasNext();){
                String key = keyIt.next();
                Float time = locs.get(key);
                int i=0;
                boolean findFlag = false;
                for(;i<posList.size();i++){
                   if( locs.get(posList.get(i)) < time){
                       findFlag = true;
                       break;
                   }
                }
                if(findFlag){
                    posList.add(i,key);
                }else{
                    posList.add(key);
                }

            }
            for(int i=0;i<3;i++){
                result.put(posList.get(i),locs.get(posList.get(i)));
            }
            return result;
        }


        /**
         * 获得位置停留信息
         */
        private HashMap<String, Float> getStayTime(TreeMap<Long, String> uploads) {
            Entry<Long, String> upload, nextUpload;
            HashMap<String, Float> locs = new HashMap<String, Float>();
            //初始化
            Iterator<Entry<Long, String>> it = uploads.entrySet().iterator();
            upload = it.next();
            //计算
            while (it.hasNext()) {
                nextUpload = it.next();
                float diff = (float) (nextUpload.getKey() - upload.getKey()) / 60.0f;
                if (diff <= 60.0)                                    //时间间隔过大则代表关机
                {
                    if (locs.containsKey(upload.getValue()))
                        locs.put(upload.getValue(), locs.get(upload.getValue()) + diff);
                    else
                        locs.put(upload.getValue(), diff);
                }
                upload = nextUpload;
            }
            return locs;
        }

    }

    @Override
    public int run(String[] args) throws Exception {
//        Configuration conf = getConf();
//
//        conf.set("date", args[2]);
//        conf.set("timepoint", args[3]);

        JobConf job = new JobConf(BaseStationDataPreprocess.class);
        job.set("timepoint", args[3]);
        job.set("date", args[2]);
        job.setJobName("BaseStationDataPreprocess");
        job.setJarByClass(BaseStationDataPreprocess.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));            //输入路径
        FileOutputFormat.setOutputPath(job, new Path(args[1]));        //输出路径

        job.setMapperClass(Map.class);                                //调用上面Map类作为Map任务代码
        job.setReducerClass(Reduce.class);                            //调用上面Reduce类作为Reduce任务代码
//        job.setOutputFormatClass(TextOutputFormat.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);

        job.setOutputFormat(TextOutputFormat.class);

        JobClient.runJob(job);
        return 0;
        //   return job.isSuccessful() ? 0 : 1;
    }

    public static void main(String[] args) throws Exception {
        System.err.println(" args size " + args.length);
        for (int i = 0; i < args.length; i++) {
            System.err.println(" args  " + i + " value " + args[i]);
        }
        if (args.length != 4) {
            System.err.println("");
            System.err.println("Usage: BaseStationDataPreprocess < input path > < output path > < date > < timepoint >");
            System.err.println("Example: BaseStationDataPreprocess /user/james/Base /user/james/Output 2012-09-12 07-09-17-24");
            System.err.println("Warning: Timepoints should be begined with a 0+ two digit number and the last timepoint should be 24");
            System.err.println("Counter:");
            System.err.println("\t" + "TIMESKIP" + "\t" + "Lines which contain wrong date format");
            System.err.println("\t" + "OUTOFTIMESKIP" + "\t" + "Lines which contain times that out of range");
            System.err.println("\t" + "LINESKIP" + "\t" + "Lines which are invalid");
            System.err.println("\t" + "USERSKIP" + "\t" + "Users in some time are invalid");
            System.exit(-1);
        }

        //运行任务
        int res = ToolRunner.run(new Configuration(), new BaseStationDataPreprocess(), args);

        System.exit(res);
    }
}